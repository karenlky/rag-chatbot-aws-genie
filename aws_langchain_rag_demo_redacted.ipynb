{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Demo:\n",
    " A demo that loads a list of HTML URLs into Vector Store (Opensearch) with Bedrock Embdedding. Then QnA using LLM with RetrievalQA chain provided by LangChain. Inference using LLaMa-2-13b from SageMaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RAG\n",
    "- Prepare HTML links\n",
    "- Store into Vector store \n",
    "- QnA using LLM with RetrievalQA chain provided by LangChain\n",
    "\n",
    "Ref: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -qqq install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\" \\\n",
    "    \"sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain==0.0.309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set True to enable debug mode\n",
    "import langchain\n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "AWS_REGION = 'us-east-1'\n",
    "\n",
    "# TODO: access as a IAM user. Ensure user has the appropriate permissions and store the access keys here\n",
    "os.environ['AWS_DEFAULT_REGION']=AWS_REGION\n",
    "os.environ['AWS_ACCESS_KEY_ID']=''\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Init LLM model (Llama-2 70b chat on Amazon Sagemaker JumpStart )\n",
    "In this demo, we use Llama-2 13b chat model as LLM Foundation model, hosted by Amazon Sagemaker JumpStart Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deploy Sagemaker endpoint model from console, then define Sagemaker endpoint model name here\n",
    "SAGEMAKER_LLM_MODEL_NAME = 'jumpstart-dft-meta-textgeneration-llama-2-13b-f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "def init_llm_sm_endpoint():\n",
    "\n",
    "    endpoint_name = SAGEMAKER_LLM_MODEL_NAME\n",
    "    aws_region=AWS_REGION\n",
    "    parameters = {\"max_new_tokens\": 1000, \"temperature\": 0.1}\n",
    "\n",
    "    class ContentHandler(LLMContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        # LLAMA-2 chat\n",
    "        # note: the input format requirement differs from LLM to LLM\n",
    "        def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "            input_str = json.dumps({\"inputs\" : [[{\"role\" : \"system\",\n",
    "            \"content\" : \"\"\"\"You are QnA bot to answer the questions based on the context. If it is not in the context, start your reply with \\\"Based on context, I cannot answer\\\"\n",
    "                            \"\"\"},\n",
    "            {\"role\" : \"user\", \"content\" : prompt}]],\n",
    "            \"parameters\" : {**model_kwargs}})\n",
    "            return input_str.encode('utf-8')\n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            return response_json[0][\"generation\"][\"content\"]\n",
    "        \n",
    "    content_handler = ContentHandler()\n",
    "\n",
    "    sm_llm = SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=parameters,\n",
    "        content_handler=content_handler,\n",
    "        endpoint_kwargs={\"CustomAttributes\": \"accept_eula=true\"},\n",
    "    )\n",
    "    return sm_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "llm = init_llm_sm_endpoint()\n",
    "text = \"What is Amazon Bedrock?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding (Amazon Bedrock Embeddings)\n",
    "We use Amazon Bedrock Titan Embedding as Embedding Foundation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "import boto3\n",
    "\n",
    "def init_eb_bedrock():\n",
    "    bedrock_client = boto3.client('bedrock-runtime',\n",
    "                                region_name=AWS_REGION,\n",
    "                                aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                                aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "    # TODO: allow Bedrock Titan access on the AWS console, and store model ID here\n",
    "    modelId = \"amazon.titan-embed-text-v1\"\n",
    "    bedrock_embeddings = BedrockEmbeddings(\n",
    "        client=bedrock_client,\n",
    "        region_name=AWS_REGION,\n",
    "        model_id=modelId, \n",
    "    )\n",
    "\n",
    "    return bedrock_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test for embedding\n",
    "bedrock_embeddings = init_eb_bedrock()\n",
    "print(bedrock_embeddings.model_id)\n",
    "embedding_vectors = bedrock_embeddings.embed_documents(['hello', 'world'])\n",
    "print(\"len(embedding_vectors): \", len(embedding_vectors))\n",
    "print(\"sample vector:\\n\",embedding_vectors[0][0:10])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Store (Amazon Opensearch service)\n",
    "We used Amazon Opensearch service as the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a Amazon OpenSearch cluster and store the URL here\n",
    "OPENSEARCH_URL = \"\"\n",
    "OPENSEARCH_VECTOR_INDEX_HTML = \"rag-demo-aws-genai-html-asia\"\n",
    "AWS_REGION_AOS = \"ap-southeast-1\"\n",
    "\n",
    "# TODO: store credentials for Amazon OpenSearch cluster (\"login\", \"password\") here\n",
    "OPENSEARCH_http_auth=(\"\", \"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split HTML into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # TODO: feel free to change chunk_size to fit context window size of LLM\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "\n",
    "# TODO: store the list of URLs you want to turn into embeddings into a txt file and store the name here\n",
    "with open('') as f:\n",
    "    urls = f.read().splitlines()\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "contents_transformed, metadatas = [], []\n",
    "for doc_transformed in docs_transformed:\n",
    "    content_transformed = text_splitter.split_text(doc_transformed.page_content)\n",
    "    contents_transformed.extend(content_transformed)\n",
    "    metadatas.extend([doc_transformed.metadata]*len(content_transformed))\n",
    "\n",
    "print(\"sample split:\\n\", contents_transformed)\n",
    "print(\"sample metadatas:\\n\", metadatas)\n",
    "print(\"length of sample split:\\n\", len(contents_transformed))\n",
    "print(\"length of sample metadatas:\\n\", len(metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Embedding and Store into Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "\n",
    "service = 'es' # must set the service as 'aoss' for Amazon OpenSearch Serverless\n",
    "region = AWS_REGION_AOS\n",
    "\n",
    "vectorstore = OpenSearchVectorSearch.from_texts(\n",
    "    contents_transformed,\n",
    "    init_eb_bedrock(), # embedding model\n",
    "    metadatas,\n",
    "    opensearch_url=OPENSEARCH_URL,\n",
    "    http_auth=OPENSEARCH_http_auth,\n",
    "    timeout = 600,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=OPENSEARCH_VECTOR_INDEX_HTML,\n",
    "    engine=\"faiss\",\n",
    "    bulk_size=10000\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "vectorstore = OpenSearchVectorSearch(\n",
    "    opensearch_url=OPENSEARCH_URL,\n",
    "    index_name=OPENSEARCH_VECTOR_INDEX_HTML,\n",
    "    embedding_function=init_eb_bedrock(),\n",
    "    http_auth=OPENSEARCH_http_auth,\n",
    "    timeout = 600,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    engine=\"faiss\"\n",
    ")\n",
    "\n",
    "\n",
    "question = \"What is a GenAI use case to improve customer experience?\"\n",
    "docs = vectorstore.similarity_search(question, k=20)\n",
    "print(f\"Vector search: Number of document related to the question = {len(docs)}\")\n",
    "print('sample result:\\n', docs[1])\n",
    "print('sample result:\\n', docs[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QnA the content using RetrievalQA chain\n",
    "QA using a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def find_metadata_sources_from_documents(documents):\n",
    "    source_list = []\n",
    "    for document in documents:\n",
    "        if 'source' in document.metadata:\n",
    "            source = document.metadata['source']\n",
    "            source_list.append(source)\n",
    "    #dedup\n",
    "    source_list = list(dict.fromkeys(source_list))\n",
    "    return source_list\n",
    "\n",
    "llm = init_llm_sm_endpoint()\n",
    "retreiver = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={'k': 6, 'score_threshold': 0.8})\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                       retriever=retreiver,\n",
    "                                       return_source_documents=True)\n",
    "\n",
    "# test\n",
    "question = \"What is Amazon Bedrock?\"\n",
    "full_result = qa_chain({\"query\": question})\n",
    "result = full_result['result']\n",
    "print(result)\n",
    "print(\"\\nsource:\\n\", find_metadata_sources_from_documents(full_result['source_documents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question that are not related to the documents\n",
    "question = \"Why Siu mei is the best Dim sum in Hong Kong?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(\"answer:\\n\", result['result'])\n",
    "print(\"\\nsource:\\n\", find_metadata_sources_from_documents(result['source_documents']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
